# -*- coding: utf-8 -*-
"""Resume analyzer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gg0rdO0HTYyudRv5D8chUbRkghhR1q4n
"""

pip install pandas openpyxl PyPDF2 openai tqdm

import os
from pathlib import Path
import pandas as pd
import PyPDF2
import re
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import logging
from typing import Dict, List, Any

class ResumeAnalyzer:
    def __init__(self):
        self.setup_logging()
        self.setup_keywords()

    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            filename='resume_processing.log'
        )

    def setup_keywords(self):
        # Keywords for different categories
        self.gen_ai_keywords = {
            'basic': set(['chatgpt', 'gpt', 'generative ai', 'llm', 'large language model', 'prompt engineering']),
            'intermediate': set(['langchain', 'llamaindex', 'vector database', 'fine-tuning', 'prompt optimization']),
            'advanced': set(['rag', 'retrieval augmented generation', 'autonomous agents', 'evaluation framework'])
        }

        self.ai_ml_keywords = {
            'basic': set(['machine learning', 'deep learning', 'tensorflow', 'pytorch', 'neural network']),
            'intermediate': set(['computer vision', 'nlp', 'natural language processing', 'transformers', 'bert']),
            'advanced': set(['research paper', 'novel architecture', 'state of the art', 'sota'])
        }

        self.technical_skills = set([
            'python', 'java', 'c++', 'javascript', 'sql', 'pytorch', 'tensorflow',
            'keras', 'scikit-learn', 'pandas', 'numpy', 'machine learning',
            'deep learning', 'nlp', 'computer vision', 'docker', 'kubernetes',
            'aws', 'azure', 'gcp', 'rest api', 'git'
        ])

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF file"""
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = ' '.join(page.extract_text() for page in reader.pages)
                return text.lower()  # Convert to lowercase for better matching
        except Exception as e:
            logging.error(f"Error extracting text from {pdf_path}: {str(e)}")
            return ""

    def calculate_keyword_score(self, text: str, keywords: dict) -> int:
        """Calculate score based on keyword matches"""
        basic_matches = sum(1 for word in keywords['basic'] if word in text)
        intermediate_matches = sum(1 for word in keywords['intermediate'] if word in text)
        advanced_matches = sum(1 for word in keywords['advanced'] if word in text)

        if advanced_matches >= 1:
            return 3
        elif intermediate_matches >= 2:
            return 2
        elif basic_matches >= 1:
            return 1
        return 0

    def extract_skills(self, text: str) -> List[str]:
        """Extract technical skills from text"""
        found_skills = []
        for skill in self.technical_skills:
            if skill in text:
                found_skills.append(skill)
        return found_skills

    def extract_education(self, text: str) -> tuple:
        """Extract education details using simple pattern matching"""
        cgpa_pattern = r'(?:cgpa|gpa)[:\s]*([0-9.]+)'
        percentage_pattern = r'([0-9]{2,3})(?:\.[0-9]+)?%'

        # Extract CGPA or percentage
        cgpa_match = re.search(cgpa_pattern, text)
        percentage_match = re.search(percentage_pattern, text)

        if cgpa_match:
            grade = f"{cgpa_match.group(1)}/10"
        elif percentage_match:
            grade = f"{percentage_match.group(1)}%"
        else:
            grade = "Not found"

        # Extract year pattern (20XX)
        year_pattern = r'\b(20[0-9]{2})\b'
        years = re.findall(year_pattern, text)
        year = max(years) if years else "Not found"

        return grade, year

    def process_single_resume(self, pdf_path: Path) -> Dict[str, Any]:
        """Process a single resume and extract all relevant information"""
        try:
            # Extract text
            text = self.extract_text_from_pdf(str(pdf_path))
            if not text:
                return None

            # Extract information
            grade, year = self.extract_education(text)
            skills = self.extract_skills(text)

            # Calculate scores
            gen_ai_score = self.calculate_keyword_score(text, self.gen_ai_keywords)
            ai_ml_score = self.calculate_keyword_score(text, self.ai_ml_keywords)

            # Calculate overall score (simplified)
            skill_score = min(len(skills) * 10, 40)  # Max 40 points for skills
            exp_score = (gen_ai_score + ai_ml_score) * 10  # Max 60 points for experience
            overall_score = min(skill_score + exp_score, 100)

            # Create result dictionary
            result = {
                'Name': pdf_path.stem,  # Using filename as name
                'Contact Details': 'Contact details extraction disabled',  # For privacy
                'University': 'University extraction disabled',  # Simplified
                'Year of Study': year,
                'Course': 'Course extraction disabled',  # Simplified
                'Discipline': 'Discipline extraction disabled',  # Simplified
                'CGPA/Percentage': grade,
                'Key Skills': ', '.join(skills),
                'Gen AI Experience Score': gen_ai_score,
                'AI/ML Experience Score': ai_ml_score,
                'Supporting Information': f"Found {len(skills)} relevant technical skills",
                'Overall Score': overall_score
            }

            return result

        except Exception as e:
            logging.error(f"Error processing {pdf_path}: {str(e)}")
            logging.error(f"Exception details:", exc_info=True)
            return None

    def process_batch(self, pdf_folder: str, output_file: str, batch_size: int = 100):
        """Process resumes in batches"""
        pdf_files = list(Path(pdf_folder).glob('*.pdf'))
        if not pdf_files:
            logging.error(f"No PDF files found in {pdf_folder}")
            return

        results = []

        # Process files in batches
        for i in range(0, len(pdf_files), batch_size):
            batch = pdf_files[i:i + batch_size]

            # Process each file in the batch
            for pdf_file in tqdm(batch, desc=f"Processing batch {i//batch_size + 1}"):
                result = self.process_single_resume(pdf_file)
                if result:
                    results.append(result)

        if not results:
            logging.error("No results were generated from resume processing")
            return

        # Create DataFrame and save to Excel
        df = pd.DataFrame(results)

        # Ensure all required columns are present
        required_columns = [
            'Name', 'Contact Details', 'University', 'Year of Study',
            'Course', 'Discipline', 'CGPA/Percentage', 'Key Skills',
            'Gen AI Experience Score', 'AI/ML Experience Score',
            'Supporting Information', 'Overall Score'
        ]

        for col in required_columns:
            if col not in df.columns:
                df[col] = 'Not found'

        # Reorder columns
        df = df[required_columns]

        # Save to Excel
        df.to_excel(output_file, index=False, engine='openpyxl')
        logging.info(f"Results saved to {output_file}")
        print(f"\nProcessed {len(results)} resumes successfully. Results saved to {output_file}")

def main():
    # Configuration
    INPUT_FOLDER = "resumes"  # folder containing resume PDFs
    OUTPUT_FILE = "resume_analysis_results.xlsx"
    BATCH_SIZE = 100

    # Create analyzer instance
    analyzer = ResumeAnalyzer()

    # Process resumes
    analyzer.process_batch(INPUT_FOLDER, OUTPUT_FILE, BATCH_SIZE)

if __name__ == "__main__":
    main()

import os
from pathlib import Path
import pandas as pd
import PyPDF2
import re
import spacy
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import logging
from typing import Dict, List, Any, Tuple
import json
from collections import defaultdict

class ResumeAnalyzer:
    def __init__(self):
        self.setup_logging()
        self.setup_nlp()
        self.setup_patterns()

    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            filename='resume_processing.log'
        )

    def setup_nlp(self):
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except:
            os.system("python -m spacy download en_core_web_sm")
            self.nlp = spacy.load("en_core_web_sm")

    def setup_patterns(self):
        self.patterns = {
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'phone': r'[\+\(]?[1-9][0-9 .\-\(\)]{8,}[0-9]',
            'linkedin': r'linkedin\.com/in/[\w\-]+/?',
            'github': r'github\.com/[\w\-]+/?',
            'portfolio': r'(?:portfolio|website):\s*(https?://\S+)',
            'cgpa': r'(?:cgpa|gpa)[:\s]*([0-9.]+)(?:/[0-9.]+)?',
            'percentage': r'([0-9]{2,3})(?:\.[0-9]+)?%'
        }

        self.education_keywords = {
            'degrees': [
                'b.tech', 'b.e.', 'b.e', 'bachelor of technology', 'bachelor of engineering',
                'm.tech', 'm.e.', 'm.e', 'master of technology', 'master of engineering',
                'bca', 'mca', 'bachelor of computer applications', 'master of computer applications'
            ],
            'disciplines': [
                'computer science', 'information technology', 'electronics', 'electrical',
                'mechanical', 'civil', 'computer engineering', 'data science', 'artificial intelligence'
            ]
        }

        self.tech_keywords = {
            'gen_ai': {
                'basic': {
                    'chatgpt': 1, 'gpt-3': 1, 'gpt-4': 1, 'large language models': 1,
                    'prompt engineering': 1, 'text generation': 1
                },
                'intermediate': {
                    'langchain': 2, 'llamaindex': 2, 'vector database': 2,
                    'embeddings': 2, 'fine-tuning': 2, 'prompt optimization': 2
                },
                'advanced': {
                    'rag': 3, 'retrieval augmented generation': 3, 'agents': 3,
                    'autonomous agents': 3, 'evaluation framework': 3, 'custom llm': 3
                }
            },
            'ai_ml': {
                'basic': {
                    'machine learning': 1, 'deep learning': 1, 'neural networks': 1,
                    'tensorflow': 1, 'pytorch': 1, 'sklearn': 1
                },
                'intermediate': {
                    'computer vision': 2, 'nlp': 2, 'transformers': 2,
                    'bert': 2, 'reinforcement learning': 2, 'gan': 2
                },
                'advanced': {
                    'research paper': 3, 'novel architecture': 3, 'sota': 3,
                    'optimization techniques': 3, 'custom architecture': 3
                }
            }
        }

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = []
                for page in reader.pages:
                    content = page.extract_text()
                    if content:
                        text.append(content)
                return '\n'.join(text)
        except Exception as e:
            logging.error(f"Error extracting text from {pdf_path}: {str(e)}")
            return ""

    def extract_contact_details(self, text: str) -> Dict[str, str]:
        contact_info = {}
        for key, pattern in self.patterns.items():
            if key in ['email', 'phone', 'linkedin', 'github', 'portfolio']:
                matches = re.findall(pattern, text)
                if matches:
                    contact_info[key] = matches[0]
        return contact_info

    def extract_education(self, text: str) -> Dict[str, str]:
        education_info = {
            'university': '',
            'course': '',
            'discipline': '',
            'year': '',
            'grade': ''
        }

        sections = text.split('\n\n')
        education_section = ''

        for section in sections:
            if any(keyword in section.lower() for keyword in ['education', 'academic', 'qualification']):
                education_section = section
                break

        if education_section:
            doc = self.nlp(education_section)
            for ent in doc.ents:
                if ent.label_ == 'ORG' and any(word in ent.text.lower() for word in
                    ['university', 'college', 'institute', 'school']):
                    education_info['university'] = ent.text
                    break

        if not education_info['university']:
            doc = self.nlp(text)
            for ent in doc.ents:
                if ent.label_ == 'ORG' and any(word in ent.text.lower() for word in
                    ['university', 'college', 'institute', 'school']):
                    context = text[max(0, text.find(ent.text) - 50):min(len(text), text.find(ent.text) + 50)]
                    if any(keyword in context.lower() for keyword in ['education', 'academic', 'student', 'studying']):
                        education_info['university'] = ent.text
                        break

        text_lower = text.lower()
        for degree in self.education_keywords['degrees']:
            if degree in text_lower:
                education_info['course'] = degree
                for discipline in self.education_keywords['disciplines']:
                    if discipline in text_lower:
                        education_info['discipline'] = discipline
                        break
                break

        year_pattern = r'\b(20[0-9]{2})\b'
        years = re.findall(year_pattern, text)
        if years:
            education_info['year'] = max(years)

        cgpa_match = re.search(self.patterns['cgpa'], text_lower)
        if cgpa_match:
            education_info['grade'] = f"{cgpa_match.group(1)}/10"
        else:
            percentage_match = re.search(self.patterns['percentage'], text_lower)
            if percentage_match:
                education_info['grade'] = f"{percentage_match.group(1)}%"

        return education_info

    def analyze_skills(self, text: str) -> Tuple[List[str], int, int, Dict, Dict]:
        text_lower = text.lower()

        gen_ai_score = 0
        gen_ai_matches = defaultdict(list)
        for level, keywords in self.tech_keywords['gen_ai'].items():
            for keyword, weight in keywords.items():
                if keyword in text_lower:
                    gen_ai_matches[level].append(keyword)
                    gen_ai_score = max(gen_ai_score, weight)

        ai_ml_score = 0
        ai_ml_matches = defaultdict(list)
        for level, keywords in self.tech_keywords['ai_ml'].items():
            for keyword, weight in keywords.items():
                if keyword in text_lower:
                    ai_ml_matches[level].append(keyword)
                    ai_ml_score = max(ai_ml_score, weight)

        all_skills = set()
        for category in self.tech_keywords.values():
            for level in category.values():
                for keyword in level.keys():
                    if keyword in text_lower:
                        all_skills.add(keyword)

        return list(all_skills), gen_ai_score, ai_ml_score, dict(gen_ai_matches), dict(ai_ml_matches)

    def extract_supporting_info(self, text: str) -> str:
        sections = {
            'certifications': [],
            'internships': [],
            'projects': []
        }

        text_blocks = text.split('\n\n')

        current_section = None
        for block in text_blocks:
            block_lower = block.lower()

            if any(cert_word in block_lower for cert_word in ['certification', 'certificate']):
                current_section = 'certifications'
            elif any(intern_word in block_lower for intern_word in ['internship', 'work experience']):
                current_section = 'internships'
            elif 'project' in block_lower:
                current_section = 'projects'

            if current_section:
                lines = [line.strip() for line in block.split('\n') if line.strip()]
                if lines and not any(header in lines[0].lower() for header in ['certification', 'internship', 'project']):
                    sections[current_section].extend(lines)

        formatted_info = []
        if sections['certifications']:
            formatted_info.append("Certifications:")
            formatted_info.extend([f"- {cert}" for cert in sections['certifications']])

        if sections['internships']:
            formatted_info.append("\nInternships:")
            formatted_info.extend([f"- {intern}" for intern in sections['internships']])

        if sections['projects']:
            formatted_info.append("\nProjects:")
            formatted_info.extend([f"- {proj}" for proj in sections['projects']])

        return '\n'.join(formatted_info) if formatted_info else "No supporting information found"

    def calculate_overall_score(self, data: Dict) -> float:
        score = 0.0

        gen_ai_score = data.get('Gen AI Experience Score', 0)
        ai_ml_score = data.get('AI/ML Experience Score', 0)
        score += (gen_ai_score + ai_ml_score) * 6.67

        skills = data.get('Key Skills', '').split(', ') if data.get('Key Skills') else []
        score += min(len(skills) * 3, 30)

        cgpa_percentage = data.get('CGPA/Percentage', '')
        if cgpa_percentage:
            try:
                if '/' in cgpa_percentage:
                    cgpa = float(cgpa_percentage.split('/')[0])
                    score += (cgpa / 10.0) * 15
                else:
                    percentage = float(cgpa_percentage.strip('%'))
                    score += (percentage / 100.0) * 15
            except:
                score += 7.5

        supporting_info = data.get('Supporting Information', '')
        if supporting_info and "No supporting information found" not in supporting_info:
            item_count = supporting_info.count('\n- ')
            score += min(item_count * 2, 15)

        return round(min(score, 100), 2)

    def process_single_resume(self, pdf_path: Path) -> Dict[str, Any]:
        try:
            text = self.extract_text_from_pdf(str(pdf_path))
            if not text:
                return None

            contact_info = self.extract_contact_details(text)
            education_info = self.extract_education(text)
            skills, gen_ai_score, ai_ml_score, gen_ai_details, ai_ml_details = self.analyze_skills(text)
            supporting_info = self.extract_supporting_info(text)

            result = {
                'Name': pdf_path.stem,
                'Contact Details': json.dumps(contact_info),
                'University': education_info['university'],
                'Year of Study': education_info['year'],
                'Course': education_info['course'],
                'Discipline': education_info['discipline'],
                'CGPA/Percentage': education_info['grade'],
                'Key Skills': ', '.join(skills),
                'Gen AI Experience Score': gen_ai_score,
                'AI/ML Experience Score': ai_ml_score,
                'Supporting Information': supporting_info
            }

            result['Overall Score'] = self.calculate_overall_score(result)
            result['Detailed Analysis'] = json.dumps({
                'Gen AI Expertise': gen_ai_details,
                'AI/ML Expertise': ai_ml_details
            })

            return result
        except Exception as e:
            logging.error(f"Error processing {pdf_path}: {str(e)}")
            return None

    def process_batch(self, pdf_folder: str, output_file: str, batch_size: int = 100):
        pdf_files = list(Path(pdf_folder).glob('*.pdf'))
        if not pdf_files:
            logging.error(f"No PDF files found in {pdf_folder}")
            return

        results = []
        total_batches = (len(pdf_files) + batch_size - 1) // batch_size

        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(pdf_files))
            batch_files = pdf_files[start_idx:end_idx]

            with ThreadPoolExecutor(max_workers=min(10, len(batch_files))) as executor:
                future_to_pdf = {executor.submit(self.process_single_resume, pdf_file): pdf_file
                               for pdf_file in batch_files}

                for future in tqdm(as_completed(future_to_pdf),
                                 total=len(batch_files),
                                 desc=f"Processing batch {batch_idx + 1}/{total_batches}"):
                    result = future.result()
                    if result:
                        results.append(result)

        if not results:
            logging.error("No results were generated from resume processing")
            return

        df = pd.DataFrame(results)
        writer = pd.ExcelWriter(output_file, engine='openpyxl')
        df.to_excel(writer, index=False, sheet_name='Resume Analysis')

        workbook = writer.book
        worksheet = writer.sheets['Resume Analysis']

        for idx, col in enumerate(df.columns):
            max_length = max(df[col].astype(str).apply(len).max(), len(col))
            worksheet.column_dimensions[chr(65 + idx)].width = min(max_length + 2, 50)

        writer.close()
        logging.info(f"Results saved to {output_file}")
        print(f"\nProcessed {len(results)} resumes successfully. Results saved to {output_file}")

def main():
    # Configuration
    INPUT_FOLDER = "resumes"
    OUTPUT_FILE = "resume_analysis_results.xlsx"
    BATCH_SIZE = 100

    # Create analyzer instance
    analyzer = ResumeAnalyzer()

    # Process resumes
    analyzer.process_batch(INPUT_FOLDER, OUTPUT_FILE, BATCH_SIZE)

if __name__ == "__main__":
    main()

import os
from pathlib import Path
import pandas as pd
import PyPDF2
import re
import spacy
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import logging
from typing import Dict, List, Any, Tuple
import json
from collections import defaultdict

class ResumeAnalyzer:
    def __init__(self):
        self.setup_logging()
        self.setup_nlp()
        self.setup_patterns()

    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            filename='resume_processing.log'
        )

    def setup_nlp(self):
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except:
            os.system("python -m spacy download en_core_web_sm")
            self.nlp = spacy.load("en_core_web_sm")

    def setup_patterns(self):
        self.patterns = {
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'phone': r'[\+\(]?[1-9][0-9 .\-\(\)]{8,}[0-9]',
            'linkedin': r'linkedin\.com/in/[\w\-]+/?',
            'github': r'github\.com/[\w\-]+/?',
            'portfolio': r'(?:portfolio|website):\s*(https?://\S+)',
            'cgpa': r'(?:cgpa|gpa)[:\s]*([0-9.]+)(?:/[0-9.]+)?',
            'percentage': r'([0-9]{2,3})(?:\.[0-9]+)?%'
        }

        self.education_keywords = {
            'degrees': [
                'b.tech', 'b.e.', 'b.e', 'bachelor of technology', 'bachelor of engineering',
                'm.tech', 'm.e.', 'm.e', 'master of technology', 'master of engineering',
                'bca', 'mca', 'bachelor of computer applications', 'master of computer applications'
            ],
            'disciplines': [
                'computer science', 'information technology', 'electronics', 'electrical',
                'mechanical', 'civil', 'computer engineering', 'data science', 'artificial intelligence'
            ]
        }

        self.tech_keywords = {
            'gen_ai': {
                'basic': {
                    'chatgpt': 1, 'gpt-3': 1, 'gpt-4': 1, 'large language models': 1,
                    'prompt engineering': 1, 'text generation': 1
                },
                'intermediate': {
                    'langchain': 2, 'llamaindex': 2, 'vector database': 2,
                    'embeddings': 2, 'fine-tuning': 2, 'prompt optimization': 2
                },
                'advanced': {
                    'rag': 3, 'retrieval augmented generation': 3, 'agents': 3,
                    'autonomous agents': 3, 'evaluation framework': 3, 'custom llm': 3
                }
            },
            'ai_ml': {
                'basic': {
                    'machine learning': 1, 'deep learning': 1, 'neural networks': 1,
                    'tensorflow': 1, 'pytorch': 1, 'sklearn': 1
                },
                'intermediate': {
                    'computer vision': 2, 'nlp': 2, 'transformers': 2,
                    'bert': 2, 'reinforcement learning': 2, 'gan': 2
                },
                'advanced': {
                    'research paper': 3, 'novel architecture': 3, 'sota': 3,
                    'optimization techniques': 3, 'custom architecture': 3
                }
            }
        }

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = []
                for page in reader.pages:
                    content = page.extract_text()
                    if content:
                        text.append(content)
                return '\n'.join(text)
        except Exception as e:
            logging.error(f"Error extracting text from {pdf_path}: {str(e)}")
            return ""

    def extract_contact_details(self, text: str) -> Dict[str, str]:
        contact_info = {}
        for key, pattern in self.patterns.items():
            if key in ['email', 'phone', 'linkedin', 'github', 'portfolio']:
                matches = re.findall(pattern, text)
                if matches:
                    contact_info[key] = matches[0]
        return contact_info

    def extract_education(self, text: str) -> Dict[str, str]:
        """Extract education details using section identification"""
        education_info = {
            'university': '',
            'course': '',
            'discipline': '',
            'year': '',
            'grade': ''
        }

        sections = text.lower().split('\n\n')
        education_section = ''

        # Look specifically for education section with common headers
        for i, section in enumerate(sections):
            if any(edu_header in section for edu_header in ['education', 'academic background', 'qualification']):
                # Take this section and the next one to capture full education details
                education_section = '\n\n'.join([sections[i], sections[i+1] if i+1 < len(sections) else ''])
                break

        if not education_section:
            return education_info

        # Process education section to find university
        doc = self.nlp(education_section)

        # Look for university name specifically in education section
        for ent in doc.ents:
            if ent.label_ == 'ORG' and any(word in ent.text.lower() for word in
                ['university', 'college', 'institute', 'school']):
                education_info['university'] = ent.text
                break

        # Extract course and discipline from education section
        text_lower = education_section.lower()
        for degree in self.education_keywords['degrees']:
            if degree in text_lower:
                education_info['course'] = degree
                # Look for discipline near degree mention
                for discipline in self.education_keywords['disciplines']:
                    if discipline in text_lower:
                        education_info['discipline'] = discipline
                        break
                break

        # Extract year from education section
        year_pattern = r'\b(20[0-9]{2})\b'
        years = re.findall(year_pattern, education_section)
        if years:
            education_info['year'] = max(years)

        # Extract CGPA/percentage from education section
        cgpa_match = re.search(self.patterns['cgpa'], text_lower)
        if cgpa_match:
            education_info['grade'] = f"{cgpa_match.group(1)}/10"
        else:
            percentage_match = re.search(self.patterns['percentage'], text_lower)
            if percentage_match:
                education_info['grade'] = f"{percentage_match.group(1)}%"

        return education_info

    def extract_supporting_info(self, text: str) -> str:
        """Extract supporting information with improved section detection"""
        sections = {
            'certifications': [],
            'internships': [],
            'projects': []
        }

        text_blocks = text.split('\n\n')

        # Initialize variables for section tracking
        current_section = None
        current_section_text = []

        for i, block in enumerate(text_blocks):
            block_lower = block.lower()

            # Detect section headers
            if 'certific' in block_lower or 'certification' in block_lower:
                if current_section:
                    sections[current_section].extend(current_section_text)
                current_section = 'certifications'
                current_section_text = []
            elif any(word in block_lower for word in ['internship', 'work experience', 'experience']):
                if current_section:
                    sections[current_section].extend(current_section_text)
                current_section = 'internships'
                current_section_text = []
            elif 'project' in block_lower:
                if current_section:
                    sections[current_section].extend(current_section_text)
                current_section = 'projects'
                current_section_text = []
            elif current_section:
                # Clean and process the content
                lines = block.split('\n')
                cleaned_lines = []
                for line in lines:
                    line = line.strip()
                    if line and not line.lower().startswith(('certification', 'internship', 'project')):
                        # Remove bullet points and common prefixes
                        line = re.sub(r'^[\s•\-\*]+', '', line).strip()
                        if line:
                            cleaned_lines.append(line)
                current_section_text.extend(cleaned_lines)

        # Add the last section
        if current_section and current_section_text:
            sections[current_section].extend(current_section_text)

        # Format the output
        formatted_info = []

        if sections['certifications']:
            formatted_info.append("Certifications:")
            formatted_info.extend([f"- {cert}" for cert in sections['certifications'] if cert])

        if sections['internships']:
            if formatted_info:
                formatted_info.append("")  # Add spacing between sections
            formatted_info.append("Internships:")
            formatted_info.extend([f"- {intern}" for intern in sections['internships'] if intern])

        if sections['projects']:
            if formatted_info:
                formatted_info.append("")  # Add spacing between sections
            formatted_info.append("Projects:")
            formatted_info.extend([f"- {proj}" for proj in sections['projects'] if proj])

        return '\n'.join(formatted_info) if formatted_info else "No supporting information found"

    def analyze_skills(self, text: str) -> Tuple[List[str], int, int, Dict, Dict]:
        text_lower = text.lower()

        gen_ai_score = 0
        gen_ai_matches = defaultdict(list)
        for level, keywords in self.tech_keywords['gen_ai'].items():
            for keyword, weight in keywords.items():
                if keyword in text_lower:
                    gen_ai_matches[level].append(keyword)
                    gen_ai_score = max(gen_ai_score, weight)

        ai_ml_score = 0
        ai_ml_matches = defaultdict(list)
        for level, keywords in self.tech_keywords['ai_ml'].items():
            for keyword, weight in keywords.items():
                if keyword in text_lower:
                    ai_ml_matches[level].append(keyword)
                    ai_ml_score = max(ai_ml_score, weight)

        all_skills = set()
        for category in self.tech_keywords.values():
            for level in category.values():
                for keyword in level.keys():
                    if keyword in text_lower:
                        all_skills.add(keyword)

        return list(all_skills), gen_ai_score, ai_ml_score, dict(gen_ai_matches), dict(ai_ml_matches)



    def calculate_overall_score(self, data: Dict) -> float:
        score = 0.0

        gen_ai_score = data.get('Gen AI Experience Score', 0)
        ai_ml_score = data.get('AI/ML Experience Score', 0)
        score += (gen_ai_score + ai_ml_score) * 6.67

        skills = data.get('Key Skills', '').split(', ') if data.get('Key Skills') else []
        score += min(len(skills) * 3, 30)

        cgpa_percentage = data.get('CGPA/Percentage', '')
        if cgpa_percentage:
            try:
                if '/' in cgpa_percentage:
                    cgpa = float(cgpa_percentage.split('/')[0])
                    score += (cgpa / 10.0) * 15
                else:
                    percentage = float(cgpa_percentage.strip('%'))
                    score += (percentage / 100.0) * 15
            except:
                score += 7.5

        supporting_info = data.get('Supporting Information', '')
        if supporting_info and "No supporting information found" not in supporting_info:
            item_count = supporting_info.count('\n- ')
            score += min(item_count * 2, 15)

        return round(min(score, 100), 2)

    def process_single_resume(self, pdf_path: Path) -> Dict[str, Any]:
        try:
            text = self.extract_text_from_pdf(str(pdf_path))
            if not text:
                return None

            contact_info = self.extract_contact_details(text)
            education_info = self.extract_education(text)
            skills, gen_ai_score, ai_ml_score, gen_ai_details, ai_ml_details = self.analyze_skills(text)
            supporting_info = self.extract_supporting_info(text)

            result = {
                'Name': pdf_path.stem,
                'Contact Details': json.dumps(contact_info),
                'University': education_info['university'],
                'Year of Study': education_info['year'],
                'Course': education_info['course'],
                'Discipline': education_info['discipline'],
                'CGPA/Percentage': education_info['grade'],
                'Key Skills': ', '.join(skills),
                'Gen AI Experience Score': gen_ai_score,
                'AI/ML Experience Score': ai_ml_score,
                'Supporting Information': supporting_info
            }

            result['Overall Score'] = self.calculate_overall_score(result)
            result['Detailed Analysis'] = json.dumps({
                'Gen AI Expertise': gen_ai_details,
                'AI/ML Expertise': ai_ml_details
            })

            return result
        except Exception as e:
            logging.error(f"Error processing {pdf_path}: {str(e)}")
            return None

    def process_batch(self, pdf_folder: str, output_file: str, batch_size: int = 100):
        pdf_files = list(Path(pdf_folder).glob('*.pdf'))
        if not pdf_files:
            logging.error(f"No PDF files found in {pdf_folder}")
            return

        results = []
        total_batches = (len(pdf_files) + batch_size - 1) // batch_size

        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(pdf_files))
            batch_files = pdf_files[start_idx:end_idx]

            with ThreadPoolExecutor(max_workers=min(10, len(batch_files))) as executor:
                future_to_pdf = {executor.submit(self.process_single_resume, pdf_file): pdf_file
                               for pdf_file in batch_files}

                for future in tqdm(as_completed(future_to_pdf),
                                 total=len(batch_files),
                                 desc=f"Processing batch {batch_idx + 1}/{total_batches}"):
                    result = future.result()
                    if result:
                        results.append(result)

        if not results:
            logging.error("No results were generated from resume processing")
            return

        df = pd.DataFrame(results)
        writer = pd.ExcelWriter(output_file, engine='openpyxl')
        df.to_excel(writer, index=False, sheet_name='Resume Analysis')

        workbook = writer.book
        worksheet = writer.sheets['Resume Analysis']

        for idx, col in enumerate(df.columns):
            max_length = max(df[col].astype(str).apply(len).max(), len(col))
            worksheet.column_dimensions[chr(65 + idx)].width = min(max_length + 2, 50)

        writer.close()
        logging.info(f"Results saved to {output_file}")
        print(f"\nProcessed {len(results)} resumes successfully. Results saved to {output_file}")

def main():
    # Configuration
    INPUT_FOLDER = "resumes"
    OUTPUT_FILE = "resume_analysis_results.xlsx"
    BATCH_SIZE = 100

    # Create analyzer instance
    analyzer = ResumeAnalyzer()

    # Process resumes
    analyzer.process_batch(INPUT_FOLDER, OUTPUT_FILE, BATCH_SIZE)

if __name__ == "__main__":
    main()

import os
from pathlib import Path
import pandas as pd
import PyPDF2
import re
import spacy
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import logging
from typing import Dict, List, Any, Tuple
import json
from collections import defaultdict

class ResumeAnalyzer:
    def __init__(self):
        self.setup_logging()
        self.setup_nlp()
        self.setup_patterns()

    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            filename='resume_processing.log'
        )

    def setup_nlp(self):
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except:
            os.system("python -m spacy download en_core_web_sm")
            self.nlp = spacy.load("en_core_web_sm")

    def setup_patterns(self):
        self.patterns = {
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'phone': r'[\+\(]?[1-9][0-9 .\-\(\)]{8,}[0-9]',
            'linkedin': r'linkedin\.com/in/[\w\-]+/?',
            'github': r'github\.com/[\w\-]+/?',
            'portfolio': r'(?:portfolio|website):\s*(https?://\S+)',
            'cgpa': r'(?:cgpa|gpa)[:\s]*([0-9.]+)(?:/[0-9.]+)?',
            'percentage': r'([0-9]{2,3})(?:\.[0-9]+)?%'
        }

        self.education_keywords = {
            'degrees': [
                'b.tech', 'b.e.', 'b.e', 'bachelor of technology', 'bachelor of engineering',
                'm.tech', 'm.e.', 'm.e', 'master of technology', 'master of engineering',
                'bca', 'mca', 'bachelor of computer applications', 'master of computer applications'
            ],
            'disciplines': [
                'computer science', 'information technology', 'electronics', 'electrical',
                'mechanical', 'civil', 'computer engineering', 'data science', 'artificial intelligence'
            ]
        }

        self.tech_keywords = {
            'gen_ai': {
                'basic': {
                    'chatgpt': 1, 'gpt-3': 1, 'gpt-4': 1, 'large language models': 1,
                    'prompt engineering': 1, 'text generation': 1
                },
                'intermediate': {
                    'langchain': 2, 'llamaindex': 2, 'vector database': 2,
                    'embeddings': 2, 'fine-tuning': 2, 'prompt optimization': 2
                },
                'advanced': {
                    'rag': 3, 'retrieval augmented generation': 3, 'agents': 3,
                    'autonomous agents': 3, 'evaluation framework': 3, 'custom llm': 3
                }
            },
            'ai_ml': {
                'basic': {
                    'machine learning': 1, 'deep learning': 1, 'neural networks': 1,
                    'tensorflow': 1, 'pytorch': 1, 'sklearn': 1
                },
                'intermediate': {
                    'computer vision': 2, 'nlp': 2, 'transformers': 2,
                    'bert': 2, 'reinforcement learning': 2, 'gan': 2
                },
                'advanced': {
                    'research paper': 3, 'novel architecture': 3, 'sota': 3,
                    'optimization techniques': 3, 'custom architecture': 3
                }
            }
        }

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = []
                for page in reader.pages:
                    content = page.extract_text()
                    if content:
                        text.append(content)
                return '\n'.join(text)
        except Exception as e:
            logging.error(f"Error extracting text from {pdf_path}: {str(e)}")
            return ""

    def extract_contact_details(self, text: str) -> Dict[str, str]:
        contact_info = {}
        for key, pattern in self.patterns.items():
            if key in ['email', 'phone', 'linkedin', 'github', 'portfolio']:
                matches = re.findall(pattern, text)
                if matches:
                    contact_info[key] = matches[0]
        return contact_info

    def extract_education(self, text: str) -> Dict[str, str]:
        """Extract education details with improved section and context awareness"""
        education_info = {
            'university': '',
            'course': '',
            'discipline': '',
            'year': '',
            'grade': ''
        }

        # Split text into sections
        sections = text.lower().split('\n\n')

        # First, explicitly identify the education section
        education_section = None
        education_start_idx = -1

        for idx, section in enumerate(sections):
            if any(header in section.lower() for header in ['IT','Institute','NIT','education', 'academic background', 'qualification']):
                education_start_idx = idx
                break

        if education_start_idx != -1:
            # Collect the education section and next few sections (but stop if we hit another major section)
            education_parts = []
            current_idx = education_start_idx

            while current_idx < len(sections) and current_idx < education_start_idx + 3:
                current_section = sections[current_idx].lower()

                # Stop if we hit another major section
                if current_idx != education_start_idx and any(
                    header in current_section for header in
                    ['experience', 'internship', 'project', 'skill', 'certification']
                ):
                    break

                education_parts.append(sections[current_idx])
                current_idx += 1

            education_section = '\n'.join(education_parts)

            if not education_section:
                return education_info

        # Process education section
        doc = self.nlp(education_section)

        # Look for educational institutions with better context awareness
        potential_institutions = []

        # First pass: collect all potential institutions
        for ent in doc.ents:
            if ent.label_ == 'ORG':
                text_lower = ent.text.lower()
                # Score the likelihood this is an educational institution
                score = 0

                # Check for educational keywords in the entity itself
                if any(word in text_lower for word in ['university', 'college', 'institute', 'school']):
                    score += 3

                # Check for education-related context in surrounding text (within 100 characters)
                start_idx = max(0, ent.start_char - 100)
                end_idx = min(len(education_section), ent.end_char + 100)
                context = education_section[start_idx:end_idx].lower()

                if any(word in context for word in ['degree', 'graduated', 'studying', 'student', 'batch']):
                    score += 2

                if any(word in context for word in ['cgpa', 'gpa', 'grade', 'percentage']):
                    score += 2

                if any(degree in context for degree in self.education_keywords['degrees']):
                    score += 2

                # Store the entity and its score
                potential_institutions.append((ent.text, score))

        # Select the institution with the highest score
        if potential_institutions:
            education_info['university'] = max(potential_institutions, key=lambda x: x[1])[0]

        # Rest of the extraction logic remains the same
        text_lower = education_section.lower()

        # Extract course and discipline
        for degree in self.education_keywords['degrees']:
            if degree in text_lower:
                education_info['course'] = degree
                for discipline in self.education_keywords['disciplines']:
                    if discipline in text_lower:
                        education_info['discipline'] = discipline
                        break
                break

        # Extract year
        year_pattern = r'\b(20[0-9]{2})\b'
        years = re.findall(year_pattern, education_section)
        if years:
            education_info['year'] = max(years)

        # Extract CGPA/percentage
        cgpa_match = re.search(self.patterns['cgpa'], text_lower)
        if cgpa_match:
            education_info['grade'] = f"{cgpa_match.group(1)}/10"
        else:
            percentage_match = re.search(self.patterns['percentage'], text_lower)
            if percentage_match:
                education_info['grade'] = f"{percentage_match.group(1)}%"

        return education_info


    def extract_supporting_info(self, text: str) -> str:
        """Extract supporting information with improved section detection"""
        sections = {
            'certifications': [],
            'internships': [],
            'projects': []
        }

        text_blocks = text.split('\n\n')

        # Initialize variables for section tracking
        current_section = None
        current_section_text = []

        for i, block in enumerate(text_blocks):
            block_lower = block.lower()

            # Detect section headers
            if 'certific' in block_lower or 'certification' in block_lower:
                if current_section:
                    sections[current_section].extend(current_section_text)
                current_section = 'certifications'
                current_section_text = []
            elif any(word in block_lower for word in ['internship', 'work experience', 'experience']):
                if current_section:
                    sections[current_section].extend(current_section_text)
                current_section = 'internships'
                current_section_text = []
            elif 'project' in block_lower:
                if current_section:
                    sections[current_section].extend(current_section_text)
                current_section = 'projects'
                current_section_text = []
            elif current_section:
                # Clean and process the content
                lines = block.split('\n')
                cleaned_lines = []
                for line in lines:
                    line = line.strip()
                    if line and not line.lower().startswith(('certification', 'internship', 'project')):
                        # Remove bullet points and common prefixes
                        line = re.sub(r'^[\s•\-\*]+', '', line).strip()
                        if line:
                            cleaned_lines.append(line)
                current_section_text.extend(cleaned_lines)

        # Add the last section
        if current_section and current_section_text:
            sections[current_section].extend(current_section_text)

        # Format the output
        formatted_info = []

        if sections['certifications']:
            formatted_info.append("Certifications:")
            formatted_info.extend([f"- {cert}" for cert in sections['certifications'] if cert])

        if sections['internships']:
            if formatted_info:
                formatted_info.append("")  # Add spacing between sections
            formatted_info.append("Internships:")
            formatted_info.extend([f"- {intern}" for intern in sections['internships'] if intern])

        if sections['projects']:
            if formatted_info:
                formatted_info.append("")  # Add spacing between sections
            formatted_info.append("Projects:")
            formatted_info.extend([f"- {proj}" for proj in sections['projects'] if proj])

        return '\n'.join(formatted_info) if formatted_info else "No supporting information found"

    def analyze_skills(self, text: str) -> Tuple[List[str], int, int, Dict, Dict]:
        text_lower = text.lower()

        gen_ai_score = 0
        gen_ai_matches = defaultdict(list)
        for level, keywords in self.tech_keywords['gen_ai'].items():
            for keyword, weight in keywords.items():
                if keyword in text_lower:
                    gen_ai_matches[level].append(keyword)
                    gen_ai_score = max(gen_ai_score, weight)

        ai_ml_score = 0
        ai_ml_matches = defaultdict(list)
        for level, keywords in self.tech_keywords['ai_ml'].items():
            for keyword, weight in keywords.items():
                if keyword in text_lower:
                    ai_ml_matches[level].append(keyword)
                    ai_ml_score = max(ai_ml_score, weight)

        all_skills = set()
        for category in self.tech_keywords.values():
            for level in category.values():
                for keyword in level.keys():
                    if keyword in text_lower:
                        all_skills.add(keyword)

        return list(all_skills), gen_ai_score, ai_ml_score, dict(gen_ai_matches), dict(ai_ml_matches)



    def calculate_overall_score(self, data: Dict) -> float:
        score = 0.0

        gen_ai_score = data.get('Gen AI Experience Score', 0)
        ai_ml_score = data.get('AI/ML Experience Score', 0)
        score += (gen_ai_score + ai_ml_score) * 6.67

        skills = data.get('Key Skills', '').split(', ') if data.get('Key Skills') else []
        score += min(len(skills) * 3, 30)

        cgpa_percentage = data.get('CGPA/Percentage', '')
        if cgpa_percentage:
            try:
                if '/' in cgpa_percentage:
                    cgpa = float(cgpa_percentage.split('/')[0])
                    score += (cgpa / 10.0) * 15
                else:
                    percentage = float(cgpa_percentage.strip('%'))
                    score += (percentage / 100.0) * 15
            except:
                score += 7.5

        supporting_info = data.get('Supporting Information', '')
        if supporting_info and "No supporting information found" not in supporting_info:
            item_count = supporting_info.count('\n- ')
            score += min(item_count * 2, 15)

        return round(min(score, 100), 2)

    def process_single_resume(self, pdf_path: Path) -> Dict[str, Any]:
        try:
            text = self.extract_text_from_pdf(str(pdf_path))
            if not text:
                return None

            contact_info = self.extract_contact_details(text)
            education_info = self.extract_education(text)
            skills, gen_ai_score, ai_ml_score, gen_ai_details, ai_ml_details = self.analyze_skills(text)
            supporting_info = self.extract_supporting_info(text)

            result = {
                'Name': pdf_path.stem,
                'Contact Details': json.dumps(contact_info),
                'University': education_info['university'],
                'Year of Study': education_info['year'],
                'Course': education_info['course'],
                'Discipline': education_info['discipline'],
                'CGPA/Percentage': education_info['grade'],
                'Key Skills': ', '.join(skills),
                'Gen AI Experience Score': gen_ai_score,
                'AI/ML Experience Score': ai_ml_score,
                'Supporting Information': supporting_info
            }

            result['Overall Score'] = self.calculate_overall_score(result)
            result['Detailed Analysis'] = json.dumps({
                'Gen AI Expertise': gen_ai_details,
                'AI/ML Expertise': ai_ml_details
            })

            return result
        except Exception as e:
            logging.error(f"Error processing {pdf_path}: {str(e)}")
            return None

    def process_batch(self, pdf_folder: str, output_file: str, batch_size: int = 100):
        pdf_files = list(Path(pdf_folder).glob('*.pdf'))
        if not pdf_files:
            logging.error(f"No PDF files found in {pdf_folder}")
            return

        results = []
        total_batches = (len(pdf_files) + batch_size - 1) // batch_size

        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(pdf_files))
            batch_files = pdf_files[start_idx:end_idx]

            with ThreadPoolExecutor(max_workers=min(10, len(batch_files))) as executor:
                future_to_pdf = {executor.submit(self.process_single_resume, pdf_file): pdf_file
                               for pdf_file in batch_files}

                for future in tqdm(as_completed(future_to_pdf),
                                 total=len(batch_files),
                                 desc=f"Processing batch {batch_idx + 1}/{total_batches}"):
                    result = future.result()
                    if result:
                        results.append(result)

        if not results:
            logging.error("No results were generated from resume processing")
            return

        df = pd.DataFrame(results)
        writer = pd.ExcelWriter(output_file, engine='openpyxl')
        df.to_excel(writer, index=False, sheet_name='Resume Analysis')

        workbook = writer.book
        worksheet = writer.sheets['Resume Analysis']

        for idx, col in enumerate(df.columns):
            max_length = max(df[col].astype(str).apply(len).max(), len(col))
            worksheet.column_dimensions[chr(65 + idx)].width = min(max_length + 2, 50)

        writer.close()
        logging.info(f"Results saved to {output_file}")
        print(f"\nProcessed {len(results)} resumes successfully. Results saved to {output_file}")

def main():
    # Configuration
    INPUT_FOLDER = "resumes"
    OUTPUT_FILE = "resume_analysis_results.xlsx"
    BATCH_SIZE = 100

    # Create analyzer instance
    analyzer = ResumeAnalyzer()

    # Process resumes
    analyzer.process_batch(INPUT_FOLDER, OUTPUT_FILE, BATCH_SIZE)

if __name__ == "__main__":
    main()